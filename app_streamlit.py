from __future__ import annotations

import os
from typing import List, Tuple

import streamlit as st
from dotenv import load_dotenv

# Projeto
from src.settings import SETTINGS
from src.milvus_utils import drop_collection
from src.rag import ingest_pdfs, retrieve_top_k
from src.export_pdf import export_chat_pdf

# Tenta importar backends locais; se n√£o der, desabilita "Local"
try:
    from src.llm_router import EmbeddingsCloud, EmbeddingsLocal, LLMCloud, LLMLocal
    _LOCAL_OK = True
except Exception:
    from src.llm_router import EmbeddingsCloud, LLMCloud  # s√≥ nuvem
    EmbeddingsLocal = None  # type: ignore
    LLMLocal = None  # type: ignore
    _LOCAL_OK = False

# -------------------------------------------------------
# Bootstrap
# -------------------------------------------------------
load_dotenv()
st.set_page_config(
    page_title="NUPETR/IDEMA-RN ‚Ä¢ Chat de Parecer T√©cnico",
    page_icon="üíº",
    layout="wide",
)
st.title("üíº NUPETR/IDEMA-RN ‚Äî Chat de Parecer T√©cnico (RAG + Milvus)")
st.caption(
    "Assistente ancorado em manuais internos (PDF). "
    "Preencha os filtros (Tipo de Licen√ßa/Empreendimento), envie PDFs e escolha o backend (Nuvem ou Local)."
)

# -------------------------------------------------------
# Estado
# -------------------------------------------------------
if "history" not in st.session_state:
    st.session_state.history: List[Tuple[str, str]] = []

# -------------------------------------------------------
# Sidebar ‚Äî Chaves e modo
# -------------------------------------------------------
st.sidebar.header("üîê Minha chave vs Chave do app")

key_mode = st.sidebar.radio(
    "Origem da chave OpenAI",
    ["Minha chave", "Chave do app (secrets)"],
    index=0,
)

user_key = ""
if key_mode == "Minha chave":
    user_key = st.sidebar.text_input("Cole sua chave (sk-...)", type="password")
    if user_key.strip():
        SETTINGS.openai_api_key = user_key.strip()
        os.environ["OPENAI_API_KEY"] = SETTINGS.openai_api_key
else:
    # tenta pegar dos secrets
    secret_key = st.secrets.get("OPENAI_API_KEY", "")
    if secret_key:
        SETTINGS.openai_api_key = secret_key
        os.environ["OPENAI_API_KEY"] = secret_key
        st.sidebar.success("Usando a **chave do app** (secrets).")
    else:
        st.sidebar.warning("Nenhuma OPENAI_API_KEY definida nos Secrets.")

st.sidebar.subheader("üß† Modo do modelo")
if _LOCAL_OK:
    mode = st.sidebar.radio("Escolha o modo", ["OpenAI (com chave)", "Modelo Local (sem chave)"], index=0)
else:
    mode = st.sidebar.radio("Escolha o modo", ["OpenAI (com chave)"], index=0)
    st.sidebar.info("Modelo Local indispon√≠vel neste ambiente.")

# Instancia backends conforme o modo
emb = None
llm = None
if mode == "OpenAI (com chave)":
    if not SETTINGS.openai_api_key:
        st.sidebar.error("Informe uma OPENAI_API_KEY (na caixa acima ou em Secrets) para usar o modo OpenAI.")
    else:
        emb = EmbeddingsCloud()
        llm = LLMCloud()
else:
    emb = EmbeddingsLocal()  # type: ignore
    llm = LLMLocal()  # type: ignore

# -------------------------------------------------------
# Filtros de dom√≠nio
# -------------------------------------------------------
st.sidebar.subheader("Filtros por metadados")
tipo_lic = st.sidebar.selectbox("Tipo de Licen√ßa", ["RLO", "LP", "LI", "LO", "OUTROS"], index=0)
tipo_emp = st.sidebar.selectbox("Tipo de Empreendimento", ["PO√áO", "ESTA√á√ÉO", "OLEODUTO", "BASE", "OUTROS"], index=0)

# Express√£o Milvus para filtrar
expr = f'tipo_licenca == "{tipo_lic}" && tipo_empreendimento == "{tipo_emp}"'

# -------------------------------------------------------
# PDFs + A√ß√µes
# -------------------------------------------------------
st.sidebar.subheader("üìÑ PDFs")
uploaded_files = st.sidebar.file_uploader(
    "Selecione (padr√£o: tipoLicenca_tipoEmpreendimento.pdf)",
    accept_multiple_files=True,
    type=["pdf"],
)

st.sidebar.subheader("A√ß√µes")
if st.sidebar.button("üßπ Limpar hist√≥rico"):
    st.session_state.history = []
    st.success("Hist√≥rico limpo.")

if st.sidebar.button("üóëÔ∏è Clear Collection (Milvus)"):
    try:
        drop_collection(SETTINGS.milvus_collection)
        st.success(f"Cole√ß√£o '{SETTINGS.milvus_collection}' removida.")
    except Exception as e:
        st.error(f"Falha ao remover cole√ß√£o: {e}")

# (debug opcional) Mostra URI atual lido dos secrets/env
st.sidebar.caption(f"Milvus URI em uso: {SETTINGS.milvus_uri}")

# -------------------------------------------------------
# Ingest√£o (bot√£o principal)
# -------------------------------------------------------
st.subheader("üì• Ingest√£o RAG ‚Äî PDFs Institucionais")
col_i, col_hint = st.columns([1, 1])

with col_i:
    disable_ing = emb is None or (mode.startswith("OpenAI") and not SETTINGS.openai_api_key)
    if st.button("üìå Indexar PDFs no Milvus", disabled=disable_ing or not uploaded_files):
        if not uploaded_files:
            st.warning("Envie pelo menos um PDF.")
        else:
            pairs = [(f.name, f.read()) for f in uploaded_files]
            with st.spinner("Gerando embeddings e inserindo no Milvus..."):
                try:
                    n = ingest_pdfs(
                        encoder=emb,  # type: ignore
                        files=pairs,
                        tipo_licenca=tipo_lic,
                        tipo_empreendimento=tipo_emp,
                        collection_name=SETTINGS.milvus_collection,
                    )
                    st.success(f"Indexa√ß√£o conclu√≠da: {n} trechos inseridos.")
                except Exception as e:
                    st.error(f"Falha na indexa√ß√£o: {e}")

with col_hint:
    st.info("Os documentos indexados ficam filtr√°veis por Tipo de Licen√ßa/Empreendimento.")

st.divider()

# -------------------------------------------------------
# Conversa
# -------------------------------------------------------
st.subheader("üí¨ Conversa")

# hist√≥rico
for role, msg in st.session_state.history:
    with st.chat_message(role):
        st.markdown(msg)

question = st.chat_input("Digite sua pergunta aqui...")
if question:
    st.session_state.history.append(("user", question))
    with st.chat_message("user"):
        st.markdown(question)

    final_answer = ""
    try:
        hits = retrieve_top_k(
            encoder=emb,  # type: ignore
            query=question,
            collection_name=SETTINGS.milvus_collection,
            top_k=5,
            expr=expr,
        )
        context_blocks = [h["text"] for h in hits]
        answer = llm.answer(question, context_blocks)  # type: ignore

        if hits:
            refs = "\n".join(
                f"‚Ä¢ {h['fonte']} (p.{h['pagina']}) ‚Äî {h['tipo_licenca']}/{h['tipo_empreendimento']}"
                for h in hits
            )
            final_answer = f"{answer}\n\n**Fontes consultadas:**\n{refs}"
        else:
            final_answer = answer
    except Exception as e:
        final_answer = f"Falha ao buscar/gerar resposta: {e}"

    with st.chat_message("assistant"):
        st.markdown(final_answer)
    st.session_state.history.append(("assistant", final_answer))

st.divider()
if st.button("üßæ Exportar conversa (PDF)"):
    try:
        out_path = "conversa_nupetr.pdf"
        export_chat_pdf(out_path, st.session_state.history, logo_path=None)
        with open(out_path, "rb") as f:
            st.download_button("Baixar PDF", data=f.read(), file_name=out_path, mime="application/pdf")
    except Exception as e:
        st.error(f"Falha ao exportar PDF: {e}")
